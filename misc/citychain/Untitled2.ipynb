{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import requests\n",
    "from multiprocessing.dummy import Pool as ThreadPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = ['https://www.citychain.com.hk/en/forhim',\n",
    "         'https://www.citychain.com.hk/en/forher',\n",
    "         'https://www.citychain.com.hk/en/seiko_sale',\n",
    "         'https://www.citychain.com.hk/en/casio_sale',\n",
    "         'https://www.citychain.com.hk/en/smartwatch',\n",
    "         'https://www.citychain.com.hk/en/dive-watch',\n",
    "         'https://www.citychain.com.hk/en/pair_watch',\n",
    "        ]\n",
    "initial_scraped_sites = []\n",
    "category_names = [f\"{x.split('/')[-1]}.csv\" for x in links] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-0e6a3c9ce8d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlinks\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'dr' is not defined"
     ]
    }
   ],
   "source": [
    "dr.get(links[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fetching product links from various categories...\n",
      "forhim  completed\n",
      "forher  completed\n",
      "seiko_sale  completed\n",
      "casio_sale  completed\n",
      "smartwatch  completed\n",
      "dive-watch  completed\n",
      "pair_watch  completed\n"
     ]
    }
   ],
   "source": [
    "print('fetching product links from various categories...')\n",
    "for i, link in enumerate(links):\n",
    "    site = requests.get(link)\n",
    "\n",
    "    soup = BeautifulSoup(site.content)\n",
    "\n",
    "    items = soup.findAll('div', {\"class\": \"w_pro_pic22 pro_letter\"})\n",
    "    titles = [x.text for x in soup.findAll('div', {'class': 'w_pro_text22'})]\n",
    "    prod_ids = [x.text for x in soup.findAll('div', {'class': 'w_pro_bh17'})]\n",
    "    original_prices = []\n",
    "    sale_prices = [] \n",
    "    for x in soup.findAll('div', {'class': 'w_pro_price22'}):\n",
    "        try:\n",
    "            original_prices.append(x.find('del').text)\n",
    "        except:\n",
    "            try:\n",
    "                original_prices.append(x.find('dd').text.strip())\n",
    "            except:\n",
    "                original_prices.append('N\\A')\n",
    "        try:\n",
    "            sale_prices.append(x.find('dt').text)\n",
    "        except:\n",
    "            sale_prices.append('N\\A')\n",
    "    prod_urls = [\"https://www.citychain.com.hk\" + x.find('a')['href'] for x in \n",
    "                 soup.findAll('figure', {'class':\"col-xs-6 w_pro_list22_box\"})]\n",
    "\n",
    "    dff = pd.DataFrame({\n",
    "        'ProductName': titles,\n",
    "        'ProductID': prod_ids,\n",
    "        'OriginalPrice': original_prices,\n",
    "        'SalePrice': sale_prices,\n",
    "        'ProductURL': prod_urls,\n",
    "    })\n",
    "\n",
    "    initial_scraped_sites.append(dff)\n",
    "    print(category_names[i].split('.')[0], ' completed')\n",
    "    ## saving the csv files for a particular category\n",
    "#     categ_name = f\"{link.split('/')[-1]}.csv\"\n",
    "#     dff.to_csv(categ_name, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapFunc(url):\n",
    "    try:\n",
    "        site = requests.get(url)\n",
    "        soup = BeautifulSoup(site.content)\n",
    "\n",
    "        all_images_tags = soup.find('ul', {'id':\"ulImgList\"}).findAll('li')\n",
    "        img_url_1 = all_images_tags[0].find('img')['src'].replace('48x57', '700x650')\n",
    "        try:\n",
    "            img_url_2 = all_images_tags[1].find('img')['src'].replace('48x57', '700x650')\n",
    "        except:\n",
    "            img_url_2 = 'N\\A'\n",
    "        try:\n",
    "            img_url_3 = all_images_tags[2].find('img')['src'].replace('48x57', '700x650')\n",
    "        except:\n",
    "            img_url_3 = 'N\\A'\n",
    "        try:\n",
    "            img_url_4 = all_images_tags[3].find('img')['src'].replace('48x57', '700x650')\n",
    "        except:\n",
    "            img_url_4 = 'N\\A'\n",
    "        try:\n",
    "            img_url_5 = all_images_tags[4].find('img')['src'].replace('48x57', '700x650')\n",
    "        except:\n",
    "            img_url_5 = 'N\\A'\n",
    "    \n",
    "        table1 = soup.findAll('table', {\"class\": \"product-details-table\"})[1]\n",
    "        table1_text = table1.getText(separator=\" \")\n",
    "\n",
    "        ind1 = table1_text.find('CASE') + 5\n",
    "        ind2 = table1_text.find('DIAL')\n",
    "        case = table1_text[ind1:ind2].replace('\\t', '').strip()\n",
    "\n",
    "        ind1 = table1_text.find('DIAL') + 4\n",
    "        ind2 = table1_text.find('STRAP')\n",
    "        dial = table1_text[ind1:ind2].replace('\\t', '').strip()\n",
    "\n",
    "        ind1 = table1_text.find('STRAP') + 5\n",
    "        strap = table1_text[ind1:].replace('\\t', '').strip()\n",
    "\n",
    "        try:\n",
    "            table2 = soup.findAll('table', {\"class\": \"product-details-table\"})[2]\n",
    "            table2_text = table2.getText(separator=\" \")\n",
    "            try:\n",
    "                ind1 = table2_text.find('MOVEMENT')+8\n",
    "                ind2 = table2_text.find('WATER RESISTANT')\n",
    "                movement = table2_text[ind1:ind2].replace('\\t', '').strip()\n",
    "            except:\n",
    "                movement = 'N\\A'\n",
    "                \n",
    "            try:\n",
    "                ind1 = table2_text.find('WATER RESISTANT')+18\n",
    "                ind2 = table2_text.find('ESTIMATED')\n",
    "                water_resist = table2_text[ind1:ind2].replace('\\t', '').strip()\n",
    "                if ind2 == -1:\n",
    "                    water_resist = table2_text[ind1:].replace('\\t', '').strip()\n",
    "            except:\n",
    "                water_resist = 'N\\A'\n",
    "        except:\n",
    "            movement = 'N\\A'\n",
    "            water_resist = 'N\\A'\n",
    "    \n",
    "        cases.append(case); dials.append(dial) ; straps.append(strap)\n",
    "        movements.append(movement); water_resists.append(water_resist)\n",
    "        product_urls_done.append(url)\n",
    "        img_urls_1.append(img_url_1); img_urls_2.append(img_url_2); img_urls_3.append(img_url_3);\n",
    "        img_urls_4.append(img_url_4); img_urls_5.append(img_url_5);\n",
    "\n",
    "    except Exception as ex:\n",
    "        print(url, '==>>', ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fetching every product's details.. (might take a while)\n"
     ]
    }
   ],
   "source": [
    "print('Now fetching every product\\'s details.. (might take a while)' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forhim.csv  successfully scraped and stored\n",
      "forhim.csv  successfully scraped and stored\n",
      "forhim.csv  successfully scraped and stored\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-4270fad210d6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mThreadPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscrapFunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mproduct_urls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         df2 = pd.DataFrame({\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    266\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    267\u001b[0m         '''\n\u001b[1;32m--> 268\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    269\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 651\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    652\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    653\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    646\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    647\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 648\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, df1 in enumerate(initial_scraped_sites[:]):\n",
    "    try:\n",
    "\n",
    "        cases, straps, dials, movements, water_resists = [], [], [], [], []\n",
    "        product_urls_done = [] \n",
    "        img_urls_1, img_urls_2, img_urls_3, img_urls_4, img_urls_5 = [], [], [], [], []\n",
    "        \n",
    "        product_urls = list(df1['ProductURL'])\n",
    "\n",
    "        cases, straps, dials, movements, water_resists = [], [], [], [], []\n",
    "        product_urls_done = [] \n",
    "        img_urls_1, img_urls_2, img_urls_3, img_urls_4, img_urls_5 = [], [], [], [], []\n",
    "\n",
    "        pool = ThreadPool(20)\n",
    "        _ = pool.map(scrapFunc, product_urls)\n",
    "\n",
    "        df2 = pd.DataFrame({\n",
    "            'ProductURL': product_urls_done,\n",
    "            'Case': cases,\n",
    "            'Dial': dials,\n",
    "            'Strap': straps,\n",
    "            'Movement': movements,\n",
    "            'WaterResistant': water_resists,\n",
    "            'img_url_1': img_urls_1,\n",
    "            'img_url_2': img_urls_2,\n",
    "            'img_url_3': img_urls_3,\n",
    "            'img_url_4': img_urls_4,\n",
    "            'img_url_5': img_urls_5,\n",
    "        })\n",
    "\n",
    "        df = pd.merge(df1, df2, on=\"ProductURL\")\n",
    "        df.to_csv(category_names[i] , index=False)\n",
    "        \n",
    "        del df1, df2, df\n",
    "        \n",
    "        print(category_names[i], \" successfully scraped and stored\")\n",
    "    \n",
    "    except Exception as ex:\n",
    "        print('!!!error occured!!! ', file, ex)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
