{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "falling-chapel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing.dummy import Pool as ThreadPool\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36 Edg/89.0.774.63',\n",
    "          }\n",
    "\n",
    "\n",
    "print('Code is running. May take few minutes ...')\n",
    "\n",
    "\n",
    "# fetcthing only all of the  products URLS\n",
    "prod_urls = []\n",
    "for page_num in range(1, 3):\n",
    "    url = f'https://www.citiwide-online.com/categories/orient-classic-series?limit=72&page={page_num}'\n",
    "    site = requests.get(url)\n",
    "    soup = BeautifulSoup(site.content, 'lxml')\n",
    "    prod_urls += [\"https://www.citiwide-online.com\"+x.find('a')['href'] for x in soup.findAll('product-item')]\n",
    "    \n",
    "    \n",
    "name_array, original_price_array, discount_price_array = [], [], []\n",
    "img_urls_array, description_html_array, description_text_array = [], [], []\n",
    "prod_urls_array = []\n",
    "\n",
    "# this function will scrap the products' pages and append information to above arrays\n",
    "def scrapify(url):\n",
    "    site = requests.get(url, headers=headers)\n",
    "    soup = BeautifulSoup(site.content)\n",
    "    title = soup.find('div', {'class':'title global-primary dark-primary'}).getText('', True)\n",
    "    orignal_price = soup.find('div', {'class':'global-primary dark-primary price-regular price js-price price-crossed'}).getText('', True)\n",
    "    discount_price = soup.find('div', {'class':'price-sale price js-price'}).getText('', True)\n",
    "    description_text = soup.find('div', {'class':'global-secondary dark-secondary description-container'}).getText('', True)\n",
    "    description_html = soup.find('div', {'class':'global-secondary dark-secondary description-container'})\n",
    "    img_urls = []\n",
    "    for temp in str(soup).split('detail_image_url')[1:]:\n",
    "        img_urls.append(temp.split('?\\\\')[0][5:])\n",
    "        \n",
    "    \n",
    "    name_array.append(title)\n",
    "    original_price_array.append(orignal_price)\n",
    "    discount_price_array.append(discount_price)\n",
    "    description_text_array.append(description_text)\n",
    "    description_html_array.append(description_html)\n",
    "    img_urls_array.append(img_urls)\n",
    "    prod_urls_array.append(url)\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# for product_url in prod_urls[:5]:\n",
    "#     scrapify(product_url, url)\n",
    "    \n",
    "# these 2 lines perform the above loop operation but does ~20 products in parallel simultaneously\n",
    "pool = ThreadPool(20)\n",
    "_ = pool.map(scrapify, prod_urls[:10])\n",
    "\n",
    "\n",
    "\n",
    "#creating dataframe object\n",
    "df = pd.DataFrame(columns=['Handle', 'Title', 'Description(HTML)', 'Description(text)','OriginalPrice',\n",
    "                           'DiscountedPrice', 'ProductURL', 'Image Src', 'Image Position'\n",
    "                            ])\n",
    "\n",
    "#preparing according to shopify import template\n",
    "for index, imgs in enumerate(img_urls_array):\n",
    "    for i, src in enumerate(imgs):\n",
    "        if i== 0:\n",
    "            df = df.append({\n",
    "                'Handle': name_array[index],\n",
    "                'Title': name_array[index],\n",
    "                'Description(HTML)': description_html_array[index],\n",
    "                'Description(text)': description_text_array[index],\n",
    "                'OriginalPrice': original_price_array[index],\n",
    "                'DiscountedPrice': discount_price_array[index],\n",
    "                'ProductURL': prod_urls_array[index],\n",
    "                'Image Src': src,\n",
    "                'Image Position': i+1\n",
    "            }, ignore_index = True)\n",
    "        else:\n",
    "            df = df.append({\n",
    "                'Handle': name_array[index],\n",
    "                'Title': '',\n",
    "                'Description(HTML)': '',\n",
    "                'Description(text)': '',\n",
    "                'OriginalPrice':  '',\n",
    "                'DiscountedPrice': '',\n",
    "                'ProductURL': '',\n",
    "                'Image Src': src,\n",
    "                'Image Position': i+1\n",
    "            }, ignore_index = True)\n",
    "            \n",
    "            \n",
    "df.to_csv('citywide - orient1.csv', index=False)\n",
    "\n",
    "df.to_excel('citywide - orient1.xlsx', index=False)\n",
    "\n",
    "\n",
    "print('Results saved in files. You may exit the code now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-celebration",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
